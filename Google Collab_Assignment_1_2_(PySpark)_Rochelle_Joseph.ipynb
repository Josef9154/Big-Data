{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1.2 (PySpark) Rochelle_Joseph.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6gHyb12iTVxECMA2vsejR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josef9154/Big-Data/blob/main/Google%20Collab_Assignment_1_2_(PySpark)_Rochelle_Joseph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK0vLGolzuUX"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv_0SwZKz1cN",
        "outputId": "f30d3fe1-0c90-45e1-8820-3de5d2bef9e0"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "\n",
        "\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "import sys\n",
        "from random import random\n",
        "from operator import add\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "        Usage: pi [partitions]\n",
        "    \"\"\"\n",
        "    spark = SparkSession\\\n",
        "        .builder\\\n",
        "        .appName(\"PythonPi\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "    partitions =  2\n",
        "    n = 100000 * partitions\n",
        "\n",
        "    def f(_):\n",
        "        x = random() * 2 - 1\n",
        "        y = random() * 2 - 1\n",
        "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
        "\n",
        "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
        "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
        "\n",
        "    spark.stop()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pi is roughly 3.140520\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}